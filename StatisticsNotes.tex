\documentclass{book}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                                                                                             %%
%%                                              Copyright 2012 Nicholas Capo                                             %%
%%                              This work is made available under the terms of the                             %%
%%                 Creative Commons Attribution-ShareAlike 3.0 Unported License.                   %%
%%                      http://creativecommons.org/licenses/by-sa/3.0/deed.en_US                       %%
%%                                                                                                                                              %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{setspace}
\usepackage{listings}
\usepackage{appendix}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[american]{babel}
\usepackage{pythontex}
\usepackage{graphicx}
\usepackage[hyperref=true, backref=true, backend=bibtex8]{biblatex}
\usepackage{csquotes}
\usepackage[pdftex, pdfusetitle, colorlinks, 
		urlcolor=blue, 
		filecolor=blue, 
		linkcolor=blue,
		citecolor=blue,]{hyperref}
\usepackage{datetime}
\settimeformat{ampmtime}

\bibliography{../textbook}

\title{\textsc{Class Notes\\ for \\ Statistics\\ Letu Math--3403}}
\author{Nicholas Capo\\ \href{mailto:nicholas.capo@gmail.com}{nicholas.capo@gmail.com}}

\date{\today\\ \currenttime}

\bibliography{textbook}

\newcommand{\note}[1]{\marginpar{\emph{Note: #1}}}

%%%%%%%%%%%%%%%
\begin{document}
\maketitle

\section*{}
\begin{tabular}{p{.8\textwidth}}
This document comprises classroom notes from {Statistics Class} at \href{letu.edu}{LeTourneau University}, in the Fall of 2012.\\

\vspace{5pt}

Although the author will attempt to be complete and correct in these notes, it is the reader's responsibility to learn and understand the material. The author assumes no responsibility for the completeness or accuracy of this document. \\

\vspace{20pt}

If you have any suggestions  or corrections feel free to email the author at \href{mailto:nicholas.capo@gmail.com}{nicholas.capo@gmail.com}\\

\vspace{5pt}

The latest version of this document is available at:\\ \url{https://bitbucket.org/nicholascapo/statisticsnotes/src/tip/StatisticsNotes.pdf}\\

\vspace{5pt}

You may also view the \LaTeX\ source code at\\
\url{https://bitbucket.org/nicholascapo/statisticsnotes}\\
\end{tabular}

\vfill

\begin{tabular}{c}
\href{http://creativecommons.org/licenses/by-sa/3.0}{\includegraphics[width=.25\textwidth]{by-sa.png}}\\
Copyright \copyright\ 2012 Nicholas Capo\\
This work is made available under the terms of the\\
Creative Commons Attribution-ShareAlike 3.0 Unported License.\\
\url{http://creativecommons.org/licenses/by-sa/3.0/deed.en_US}\\
\end{tabular}

%%%%%%%%%%%
\tableofcontents
%%%%%%%%%%%

\chapter{Introduction}
\begin{center}
\textbf{Definition of Statistics}\\
\enquote{Statistics is the science of collecting, organizing, analyzing, and interpreting data in order to make decisions.}
\end{center}

\section{Data}

\subsection{Data Sets}
\begin{description}
\item[Population] The collection of all outcomes, responses, measurements, or counts, that are of interest.

\item[Sample] A subset of the population.

\item[Parameter] A number that describes a population characteristic.

\item[Statistic] A number that describes a sample characteristic.
\end{description}

\subsection{Types of Data}

\begin{description}
\item[Qualitative Data] Attributes, labels, or non-numerical entries.

\item[Quantitative Data] Numerical measurements or counts.
\end{description}

\section{Sample Mean and Median}

\subsection{Definition}
\begin{description}
\item[Sample Mean] The average of the sample data points, however it may not be a data point.
$$\overline{x} = \sum_{i=1}^n\frac{x_i}{n} = \frac{x_1+x_2+x_3\cdots x_n}{n}$$
\item[Sample Median] The middle value of the data.

$$\tilde{x}=\left\{
\begin{matrix}
x_{(\frac{n+1}{2})} & \text{if $n$ is odd}\\
\frac{1}{2}(x_{\frac{n}{2}}+x_{\frac{n}{2}+1}) & \text{if $n$ is even}
\end{matrix}
\right.$$

\item[Trimmed Mean] A trimmed mean is computed by trimming off the largest and smallest set of values. For example a 10\% trimmed mean is found by eliminating the largest 10\% and smallest 10\% and computing the mean of the remaining values. This may be useful for data that contains possible outliers. Denoted by $x_{tr(\text{percent})}$
\end{description}

\section{Measures of Variability}

\subsection{Standard Deviation}

\subsubsection{Sample Variance}

$$s^2 = \sum_{i=1}^n \frac{(x_i - \overline{x})^2}{n-1}$$

\subsubsection{Sample Standard Deviation}

$$s=+\sqrt{s^2}$$

The standard deviation is $0$ when all the data points are the same.

\section{Descriptive Statistics}

\subsection{Quartiles}

Quartiles approximately divide an ordered data set into four equal parts.

\begin{description}
\item[First Quartile, $Q_1$]
About $25\%$ of the data fall on or below $Q_1$
\item[Second Quartile, $Q_2$]
About $50\%$ of the data fall on or below $Q_2$
\item[Third Quartile, $Q_3$]
About $75\%$ of the data fall on or below $Q_3$
\end{description}

\subsection{Range and Interquartile Range}

\subsubsection{Range}

$$\text{range} = \text{max value} - \text{min value}$$

\subsubsection{Interquartile Range}

$$IQR=Q_3 - Q_1$$

To help find outliers, compute $1.5 \times IQR$, and any values that lie outside the interval $[Q_1-1.5 \times IQR, Q_3+1.5 \times IQR]$ is a possible (and probable) outlier.

\subsection{Box and Whisker Plot}

Exploratory Data Analysis Tool

\begin{itemize}
\item Requires
	\begin{itemize}
	\item Min
	\item $Q_1$
	\item Median
	\item $Q_3$
	\item Max
	\end{itemize}
\end{itemize}

\begin{pycode}
import pylab
data = [1, 2, 3, 4, 5, 6, 11]
pylab.figure(figsize=(5,2))
pylab.boxplot(data, vert=0, sym='bx')
pylab.savefig('whiskerplot.pdf', bbox_inches='tight', orientation='landscape')
sdata = sorted(data)
min = sdata[0]
outlier = sdata[-1]
max = sdata[-2]
median = pylab.median(sdata)

\end{pycode}

\subsubsection{Example}

\begin{tabular}{ll}
Example Data &\py{data}\\
Min &\py{min}\\
Median & \py{median}\\
Max & \py{max}\\
Outlier & \py{outlier}\\
\end{tabular}


\begin{figure}[H]
\begin{center}
\includegraphics[width=.75\textwidth]{whiskerplot}
\end{center}
\caption{Example Box And Whisker Plot}
\end{figure}

\section{Stem and Leaf Plots}

These look like a sideways histogram

Data: \py{[31, 21, 32, 33, 41, 42, 58, 25, 21]}\\

\begin{tabular}{r|ll}
Stem & Leaf & Key: $a|b=ab$\\
\hline
2&1,1,5\\
3&1,2,3\\
4&1,2\\
5&8\\
\end{tabular}

\subsection{Key Notation}

Key: 4|5 = 45
Key: 4|5 = 4.5

\subsection{Double Stem and Leaf}

Separate the leaves into two groups, (0-4, and 5-9)

Data: \py{[31, 21, 32, 33, 41, 42, 58, 25, 21]}\\

\begin{tabular}{r|ll}
Stem & Leaf & Key: $a|b=ab$\\
\hline
2&1,1\\
2&5\\
3&1,2,3\\
4&1.2\\
4&\\
5&\\
5&8\\
\end{tabular}

\section{Frequency Distribution}
A table that shows classes or intervals of data with a count of the number of entries in each class.

\subsection{Midpoint of a Class}
Average of the class limits.
$$\frac{(\text{lower class limit})+(\text{upper class limit})}{2}$$

\subsection{Relative Frequency}
$$\frac{\text{class frequency}}{\text{sample size}}=\frac{f}{n}$$

\section{Scatter Plots}
Each entry in one data set corresponds to one entry in a second set, one-to-one mapping.

\subsection{Example Scatter Plot}

\begin{pycode}
import pylab
import random
count = 12
data = []
for i in range(count): data.append(random.randint(1, count))
x = range(1, count+1)

pylab.figure(figsize=(5, 5))

pylab.scatter(x, data, label='$(x, y)$')

pylab.legend()
pylab.savefig('scatter.pdf', orientation='landscape')
\end{pycode}

\begin{tabular}{ll}
Data:&\\
X: & \texttt{\py{x}}\\
Y: & \texttt{\py{data}}\\
\end{tabular}

\begin{figure}[H]
\begin{center}
\includegraphics[width=.75\textwidth]{scatter}
\end{center}
\caption{Example Scatter Plot}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Probability}

\section{Experiments}
Any process that generates a set of data.

\section{Sample Space}
The set of all possible outcomes of a statistical experiment, denoted $S$. The sample space with no elements is the empty set or null set, denoted $\emptyset$

\subsection{Example}

$$S = \{ 3, 2, 1, 0\}$$
$$S = \{ x | 0 < x < 25 \}$$
$$S= \{ x^2 | x \in \mathbb{R}\}$$

\subsection{Tree Diagrams}
A Tree Diagram can be used to list all possible outcomes

\subsection{Events}
An event is a subset of a sample space. The null set ($\emptyset$) and the sample space ($S$) are both subsets of the sample space $S$.

\subsubsection{Intersection}
The intersection of two events $A$ and $B$, denoted $A \cap B$, is the event containing all elements that are common to $A$ and $B$. If $A \cap B = \emptyset$ than $A$ and $B$ are called mutually exclusive or disjoint.

\subsubsection{Union}
The union of two events $A$ and $B$, denoted $A \cup B$, is the event containing all elements that belong to $A$ or $B$ or both.

\subsubsection{Compliment}
The compliment of an event $A$ with respect to $S$ is a subset of all elements of $S$ not in $A$, denoted $A'$

\section{Counting Sample Points}

\subsection{Multiplication Rule}
If an operation can be preformed in $n_1$ ways and if for each of the ways a second operation can be preformed in $n_2$ ways, then the two operations can be preformed together in $n_1n_2$ ways. This principle can be extended to more than two operations. See Example 2.14 in \textcite[45]{textbook}

\subsection{Factorial}
For any non-negative integer $n$, $n!$ called \enquote{n factorial}, is defined as $$n!=n(n-1)\cdots(2)(1)$$
with the special case $0!=1$.

\subsection{Permutation}
A permutation is an arrangement of all or a part of a set objects. For permutations the order of objects matters. The number of permutations of $n$ distinct objects is $n!$.

\subsection{Permutations at a Time}
The number of permutations of $n$ distinct objects taken $r$ at a time is
$${}_nP_r = \frac{n!}{(n-r)!}$$
\note{This is called \enquote{$n$ Permute $r$}}

\subsection{Permutations in a Circle}
The number of permutations of $n$ objects arranged in a circle is $(n-1)!$.

\subsection{Permutations of a Kind}
The number of distinct permutations of $n$ objects of which $n_1$ are of one kind, $n_2$ of a second kind, \ldots , $n_k$ of a $k$th kind is

$$\frac{n!}{n_1!n_2! \cdots n_k!}$$

\subsection{Partitioning}
The number of way of partitioning a set of $n$ objects into $k$ cells with $n_1$ elements in the first cell, $n_2$ elements in the second cell, and so forth, is
$$ {n \choose n_1, n_2, \ldots , n_k }= \frac{n!}{n_1!n_2! \cdots n_k!}$$
Where $n_1+n_2+\cdots+n_k = n$

\note{This is the same as the last example}

\subsection{Combinations}

The number of combinations of $n$ distinct object taken $r$ at a time, is:
$${}_nC_r={n \choose r} = \frac{n!}{r!(n-r)!}$$
\note{This is partitioning with only two cells}

\subsection{Partitioning and Combinations}

Note that:

$$ {10 \choose 5, 4, 1} = {10 \choose 5} {5 \choose 4} {1 \choose 1}$$


%%%%%%%%%
\section{Probability of an Event}

The probability of an event $A$ is the sum of the weifhts of all sample points on $A$. Therefore $$0 \le P(A) \le 1$$ $$P(\emptyset)=0$$ $$P(S)=1$$

Furthermore, if $A_1, A_2, \cdots$ is a set of mutually exclusive events, then $$P(A_1 \cup A_2 \cup \cdots)=P(A_1)+P(A_2)+\cdots$$

\subsection{Complimentary Probability}
If $A$ and $A'$ are complimentary events, then $$P(A)+P(A')=1$$

\subsection{Different Outcomes}

If an experiment can result in any one of $N$ different equally likely outcomes, and of exactly $n$ of those outcomes correspond to event $A$, the probability of event $A$ is $$P(A) = \frac{n}{N}$$

\subsection{Additive Rule of Probability}

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

\section{Conditional Probability}

The conditional probability of $B$ given $A$, denoted $P(B|A)$ is defined as $$P(B|A) = \frac{P(A \cap B)}{P(A)} \text{, where } P(A) \ne 0 $$
\note{This is a filter}

\subsubsection{Alternate Notation}
$$P(A|B) =\frac{n(A \cap B)}{n(B)}$$
Where $n(x)$ is defined as \enquote{the number of elements in $x$} 

\subsection{Multiplication Rule}
If in an experiment the events $A$ and $B$ can both occur, then $$P(A\cap B) = P(A)P(B|A)\text{, provided } P(A) > 0$$

\subsection{Independent Events}
Two events $A$ and $B$ are \emph{independent} if and only if 
$$P(B|A) = P(B) \text{ or } P(A|B)=P(A)$$
Assuming the existence of the conditional probabilities. Otherwise, $A$ and $B$ are dependent.

\note{Independence does not imply Mutual Exclusivity!}

\subsection{Corollary}
Two events $A$ and $B$ are \emph{independent} if and only if 
$$P(A \cap B) = P(A)P(B)$$

Therefore, to obtain the probability that two independent events will occur, we simply find the product of their individual probabilities.

\subsection{Compliments of Independent Events}
If $A$ and $B$ are independent events so are the complements of these events, this means that: 
\begin{itemize}
\item $A'$ and $B'$ are independent
\item $A'$ and $B$  are independent
\item $A$ and $B'$ are independent
\end{itemize}
\note{Also $P(A'|B) = 1 - P(A|B)$ (no need for independence)}

\subsection{Extended Multiplication for Conditional Probability}
If, in an experiment, the events $A_1, A_2 \cdots A_k$ can occur, then
$$P(A_1 \cap A_2 \cap \cdots \cap A_k) = P(A_1) \times P(A_2|A_1) \times P(A_3|A_1 \cap A_2) \times \cdots \times P(A_k |  A_1 \cap A_2 \cap \cdots \cap A_{k-1})$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayes' Rule}

\subsection{Partitioning the Sample Space}
A sample space $A$ can be partitioned into an arbitrary number of subsets. For partitions $B_1, B_2, B_3$ then $$A = (B_1 \cap A) \cup (B_2 \cap A) \cup \cdots \cup (B_3 \cap A)$$

\subsection{Theorem of Total Probability}
$$P(A) = \sum_{i=1}^k P(B_i \cap A) = \sum_{i=1}^k P(B_i)P(A|B_i)$$

\subsection{Bayes Rule}
$$P(B_i|A) 
= \frac{P(B_i \cap A)}{\sum_{i=0}^k P(B_i \cap A)} 
= \frac{P(B_i \cap A)}{\sum_{i=0}^k P(B_i)P(A|B)}$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Random Variables}

\section{Concept of a Random Variable}

\subsection{Definition}
A random variable, denoted $X$, is a function that associates a real number with each element in the sample space.

\subsection{Discrete Sample Space}
If a sample space contains a finite number of possibilities or an unending sequence with as many elements as there are whole numbers, it is called a \emph{discrete sample space}.

A random variable corresponding with a discrete sample space is called a \emph{discrete random variable}.

\subsection{Continuous Sample Space}
If a sample space contains an infinite number of possibilities equal to the number of points on a line segment, it is called a \emph{continuous sample space}.

A random variable corresponding with a continuous sample space is called a \emph{continuous random variable}.

\section{Discrete Probability Distribution}
The set of ordered pairs $(x, f(x))$ is a \emph{probability function}, \emph{probability mass function}, or \emph{probability distribution} of the discrete random variable $X$ if, for each possible outcome $x$, 
\begin{enumerate}
\item $f(x) \ge 0$
\item $\sum_x f(x) = 1$
\item $P(X = x) = f(x)$
\end{enumerate}
\note{Probabilities are Functions!} 

\subsection{Cumulative Distribution Function}

The Cumulative Distribution Function $F(x)$ of a discrete random variable$X$ with probability distribution $f(x)$ is
$$F(x)=P(X \le x) = \sum_{t\le x}F(t)\text{, for } -\infty < x < \infty$$

\note{This keeps a running total of the cumulative probabilities up to a value}

\section{Continuous Random Variables}

\subsection{Probability Density Function}
The function $f(x)$ is called a probability density function or pdf for the continuous random variable $X$, defined over the set of real numbers, if
\begin{enumerate}
\item $$F(x) \ge 0 \text{, for all } x \in R$$
\item $$\int_{-\infty}^{\infty}f(x) \text{dx} = 1$$
\item $$P(a < X < b) = \int_a^b f(x) \text{dx}$$
\end{enumerate}
\note{$P(X=c)=0$, where $a \le c \le b$}

\subsection{Cumulative Distribution Function}
The Cumulative Distribution Function $F(x)$ of a continuous random variable $X$ with density function $f(x)$ is
$$F(x) = P(X \le x) = \int_{-\infty}^x f(t) \text{ dt, for } -\infty < x < \infty$$

\note{$P(a<X<b)=F(a)-F(b)$}
\note{$f(x)=F'(x)$}

\subsection{Joint Probability Mass Functions}
The function $f(x,y)$ is a Joint Probability Function or Probability Mass Function of the discrete random variables $X$ and $Y$ if
\begin{enumerate}
\item $$f(x,y) \ge 0 \text{ for all } (x,y)$$
\item $$\sum_x\sum_y f(x,y) =1 $$
\item $$P(X=x, Y=y)=f(x,y)$$
\item For any region $A$ in the $xy$ plane, $$P((X,Y) \in A) = \sum_A\sum f(x,y)$$

\end{enumerate}

\section{Joint Probability Distributions}
he function $f(x,y)$ is a joint density function of the continuous random variables $X$ and $Y$ if
\begin{enumerate}
\item $$f(x,y) \ge 0 \text{, for all} (x,y)$$
\item $$\int_{-\infty}^\infty \int_{-\infty}^\infty f(x,y) \text{dx dy} = 1$$
\item $$P((X,Y) \in A) = \int_A \int f(x,y) \text{dx dy}$$
\end{enumerate}

\subsection{Marginal Distribution}
The marginal distribution of $X$ alone $Y$ alone are 

Discrete
$$g(x) = \sum_y f(x,y) \text{  and  } h(y) = \sum_x f(x,y)$$

Continuous
$$g(x) = \int_{-\infty}^\infty f(x,y) \text{dy} \text{  and  } h(y) = \int_{-\infty}^\infty f(x,y) \text{dx}$$

\note{Marginal distributions are probability distributions}

\section{Joint PDFs}
Let $X$ and $Y$ be two random variables, discrete or continuous. The conditional distribution of the random variable $Y$ given that $X=x$ is
$$f(y|x) = \frac{f(x,y)}{g(x)} \text{, provided } g(x) > 0$$

Similarly, the conditional distribution of the random variable $X$ given that $Y=y$ is
$$f(y|x) = \frac{f(x,y)}{h(y)} \text{, provided } h(y) > 0$$

Where $g(x)$ and $h(y)$ are the respective marginal distributions.

\subsection{Statistical Independence}
Let $X$ and $Y$ be two random variables, discrete or continuous, with joint probability distribution $f(x,y)$ and marginal distributions $g(x)$ and $h(y)$ respectively. The random variables $X$ and $Y$ are said to be statistically independent if and only if
$$f(x,y)=g(x)h(y)$$
for all $(x,y)$ withing their range.

\subsubsection{Statistical Independence (Extended Case)}
Let $X_1, X_2, \cdots , X_n$ be n random variables, discrete or continuous, with joint probability distribution $f(x_1, x_2, \cdots , x_n)$ and marginal distributions $f_1(x_1), f_2(x_2), \cdots , f_n(x_n)$. The random variables $X_1, X_2, \cdots , X_n$ are said to be statistically independent if and only if
$$f(x_1, x_2, \cdots, x_n) = f_1(x_1), f_2(x_2), \cdots , f_n(x_n)$$
for all $(x,y)$ withing their range.

%%%%%%%%%%%%%%%%%%
\chapter{Mean of a Random Variable}

\section{Mean of a Random Variable}
Let $X$ be a random variable with probability distribution $f(x)$. The mean, or expected value, of $X$ is:

Discrete:
	$$\mu = E(X) = \sum_x x f(x)$$ 

Continuous:
	$$\mu = E(X) = \int_{-\infty}^\infty x f(x) \text{dx}$$


\subsection{Expected Values of Functions of Random Variables}

Let $X$ be a random variable with pdf $f(x)$. The expected value of the random variable $g(X)$ is

Discrete:
$$E(g(X))=\sum_x g(x)f(x)$$

Continuous:
$$E(g(X))=\int_{-\infty}^\infty g(x)f(x)$$

\subsection{Expected Values of Joint distributions}
Let $X$ and $Y$ be random variables with joint probability distribution $f(x,y)$. The mean, or expected value, of a random random variable $g(x,y)$ is

Discrete:
$$\mu_{g(x,y)}=E[g(x,y)] = \sum_X \sum_Y g(x,y)f(x,y)$$

Continuous:
$$\mu_{g(x,y)}=E[g(x,y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y)f(x,y)$$

\section{Variance and Covariance of Random Variables}

\subsection{Variance/Standard Deviation}

Variance/Standard Deviation is a measure of the typical (average) amount the value deviates from the mean.

The further the values are from the mean, the grater the variance/standard variation. 

\subsection{Variance of a Random Variable}
Let $X$ be a random variable with probability distribution $f(x)$ and mean $\mu$. The variance of $X$ is 

Discrete:
$$\sigma^2=E[(X-\mu)^2]=\sum_x(x-\mu)^2 f(x)$$

$$\sigma^2=E[(X-\mu)^2]=\int_{-\infty}^\infty (x-\mu)^2 f(x)$$

\subsubsection{Standard Variation}

$$\sigma = \sqrt{\sigma^2} = \sqrt{E((X-\mu)^2)}$$

\begin{itemize}
\item Variance --- In terms of the mean's unit squared
\item Standard Deviation --- Terms of the mean's unit
\item It does not makes sense to add or subtract the variance and the mean
\item It does not make sense to add or subtract the standard deviation and the mean
\end{itemize}

\subsection{Variance of a Random Variable}
The variance of a Random Variable $X$ is
$$\sigma^2 = E(X^2)-\mu^2$$

\subsection{The Empirical Rule}
For data with a (symmetric) bell-shaped distribution,
\begin{itemize}
\item $P[(\mu -\sigma) < X < (\mu + \sigma)] \approx 68\%$
\item $P[(\mu -2\sigma) < X < (\mu + 2\sigma)] \approx 95\%$
\item $P[(\mu -3\sigma) < X < (\mu + 3\sigma)] \approx 99.7\%$
\end{itemize}

\subsection{Chebyshev's Theorem}

The probability that any random variable $X$ will assume a value within $k$ standard deviations of the mean is at least $1-\frac{1}{k^2}$. That is,
$$P[(\mu - k\sigma) < X < (\mu + k\sigma)] \ge 1 - \frac{1}{k^2}$$

\subsection{Variance of a Function of a Random Variable}
Let $X$ be a random variable with probability distribution $f(x)$. The variance of the random variable $g(X)$ is

Discrete:
$$\sigma^2_{g(X)} = E\{[g(X) - \mu_{g(X)}]^2\} = \sum_x [g(x) - \mu_{g(X)}]^2 f(x)$$

Continuous:
$$\sigma^2_{g(X)} = E\{[g(X) - \mu_{g(X)}]^2\} = \int_{-\infty}^\infty [g(x) - \mu_{g(X)}]^2 f(x) \text{dx}$$

\subsection{Alternative Form of Variance}
$$\sigma^2 = E[[g(x)]^2]-\mu^2_{g(x)}$$

\subsection{Covariance of $X$ and $Y$}
Let $X$ and $Y$ be random variable with joint pdf  $f(x,y)$. The covariance of $X$ and $Y$ is

Discrete:
$$\sigma_{XY} = E[(X-\mu x)(Y-\mu y)]=\sum_X \sum_Y (x-\mu x)(y-\mu y) f(x,y)$$

Continuous:
$$\sigma_{XY} = E[(X-\mu x)(Y-\mu y)]=\int_{-\infty}^\infty \int_{-\infty}^\infty (x-\mu x)(y-\mu y) f(x,y) \text{dx dy}$$

\subsection{Alternate Form}
$$\sigma_{XY}=E(XY) - \mu_x \mu_y$$

\subsection{Description of Covariance}
\textbf{[See Slide 4 -- 26]}

\subsection{Correlation Coefficient}
The correlation coefficient is a unit free measure of the strength of the linear relationship between two variables

Let $X$ and $Y$ be random variables with covariance $\sigma_{XY}$, and standard deviations $\sigma_X$ and $\sigma_Y$ respectively. The correlation coefficient of X and Y is

$$\rho _{XY} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$$

For $-1 \le \rho \le 1$


\section{Linear Combinations of Random Variables}

\subsection{Expected Value of a Sum or Difference of Functions}
The expected value of the sum or difference of two or more functions of a random variable X is the sum or difference of the expected values of the functions. That is,

$$E[g(X) \pm h(X)] = E[g(X)] \pm E[h(X)].$$

\subsection{Properties of Expected Values}

\begin{itemize}
\item Where $k$ is a constant, and $X$ and $Y$ are Random Variables
\item $E(k)=k$
\item $E(kX)=kE(X)$
\item Example:
	$E(3X+5Y+2)=3E(X)+5E(Y)+2$
\end{itemize}

\subsection{}
Let $X$ and $Y$ be independent random variables. Then

$$E(XY)=E(X)E(Y)$$
\note{The inverse does not hold}

\subsection{}
If $X$ and $Y$ are random variables with joint probability distribution $f(x,y)$ and $a$, $b$, $c$ are real constants then

$$\sigma^2_{aX+bY+c}=a^2\sigma^2_X+b^2\sigma^2_Y+2ab\sigma_{XY}$$

\subsubsection{Independent Covariance}
If $X$ and $Y$ are independent random variables

$$\sigma_{XY}=0$$

\subsection{Piston Example}
[See Slide 4--43]

\chapter{Binomial and Multinomial Distributions}

\section{Introduction}
[Not included in these Notes]

\section{Bernoulli Experiment}
\begin{enumerate}
\item The experiment is repeated for a fixed number of trials, where each trial is independent of other trials
\item There are only two possible outcomes of interest: success ($S$) or failure ($F$)
\item The probability of success $p$ is the same for each trial
\item The random variable $X$ counts the number of successful trials
\end{enumerate}

\subsection{Probability Distribution}
The probability of exactly $x$ successes in $n$ trials is
$$P(X=x)={n \choose x} p^xq^{n-x}, x = 0,1,2,3, \cdots,n$$
Where:
\begin{description}
\item [$n=$] number of trials
\item [$p=$] probability of success
\item [$q=$] ($1-p$) probability of failure
\item [$x=$] number of successes in $n$ trials
\item [$n-x=$] number of failures in $n$ trials
\end{description}

\subsubsection{Binomial PDF Function}
Found in Calculator: Catalog $\rightarrow$ (F3) Flash Apps $\rightarrow$ \texttt{binomPdf(}

\subsubsection{Binomial CDF Function}
Found in Calculator: Catalog $\rightarrow$ (F3) Flash Apps $\rightarrow$ \texttt{binomCdf(}

\subsubsection{Mean and Variance of Binomial Distribution}

The mean and variance of the binomial distribution $b(x;n,p)$ are $\mu = np$ and $\sigma^2 = npq$.

\subsection{Multinomial Distribution}

Similar to binomial distribution, except the number of possible outcomes is more than two.

$$P(X_1=x_1, X_2=x_2, \cdots X_k=x_k) = {n \choose x_1, x_2, x_3 \cdots x_n} p_1^{x_1}p_2^{x_2} \cdots p_n^{x_n}$$

\begin{description}
\item[$n=$] number of trials
\item[$p_i=$] probability of the $i$\textsuperscript{th} outcome
\item [$x_i = $] number of occurrences of the $i$\textsuperscript{th} outcome in $n$ trials
\end{description}
\note{${n \choose x_1, x_2, x_3}$ $=$ $\frac{n!}{x_1! \times x_2! \times x_3!}$}

\section{Hypergeometric Distribution}

\begin{enumerate}
\item A random sample of size $n$ is selected without replacement from $N$ items. (dependent events)
\item $k$ items are classified as successes and $N-k$ items as failures
\item $x$ represents the number of successes in a random sample of size $n$
\end{enumerate}

\subsection{Hypergeometric Formula}
$$P(X=x) = \frac{{k \choose x}{N-k \choose n-x}}{{N \choose n}}$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Continuous Probability Distributions}

\section{Continuous Uniform Distribution}

Uniform: $f(x;A,B)=\frac{1}{B-A}\text{, }A \le x \le B$

\subsection{Mean and Variance}
the mean and average of the uniform distribution are
$$\mu=\frac{A+B}{2}$$
$$\sigma^2=\frac{(B-A)^2}{12}$$

\section{Normal Distribution}

Many Random Variables have distributions that can be approximated by a bell shaped curve.

The Distribution Formula is
$$n(x;\mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}}
\text{ to the power of }{e}^{\frac{-1}{2\sigma^2}(x-\mu)^2}$$
Where $-\infty < x < \infty$

\subsection{Properties of Normal Distribution}
\begin{enumerate}
\item The mean median and mode are equal
\item The normal curve approaches, but does not touch the $x$-axis. That is: Horizontal Asptote at $y=0$
\item The line of symmetry gives the location of the mean.
\item The inflection points provides a way to estimate the standard deviation from the graph
\end{enumerate}

\subsection{The Standard Normal Distribution}
The Standard Normal Distribution has a mean of 0 and a standard deviation of 1.

\subsubsection{Calculator Help}
Catalog $\rightarrow$ F3 (Flash Apps) $\rightarrow$ normCDF($x_\text{low}$, $x_\text{high}$, [$\mu$, $\sigma$])

\subsection{Conversion from Normal to Standard Normal Distributions}

Any $x$-value of a normal distribution can be transformed into a $z$-value in a standard normal distribution by
$$z=\frac{x-\mu}{\sigma}$$


\subsection{Probability and Normal Distributions}

If a random variable $x$ is normally distributed, you can find the probability that $x$ will occur in a given interval by calculating the area under the normal curve for that interval.
\addtocounter{section}{3}
\section{Gamma and Exponential Distributions}

The gamma and exponential distributions are distributions that are often used to model time to failure, waiting, times and between arrival times.

\subsection{Gamma Function}
The gamma function is defined by
$$\Gamma(\alpha) = \int_0^\infty x^{\alpha -1}e^{-x} \text{dx, for } \alpha > 0$$

If $\alpha$ is a positive integer then 
$$\Gamma(n) = (n-1)!$$

\subsection{Gamma Distribution}
The continuous random variable $X$ has a gamma distribution, with parameters $\alpha$ and $\beta$, if its density function is given by 
$$f(x;\alpha, \beta) = \left \{
\begin{matrix}
\frac{1}{\beta^\alpha\Gamma(\alpha)}x^{\alpha-1}e^{-x/\beta} &,& \text{ for } x >0\\
0 &,& \text{ elsewhere }
\end{matrix}
\right.
$$
When $\alpha = 1$ the gamma distribution becomes the exponential distribution

$$f(x, \beta) = \left \{
\begin{matrix}
\frac{1}{\beta}x^{\alpha-1}e^{-x/\beta} &,& \text{ for } x >0\\
0 &,& \text{ elsewhere }
\end{matrix}
\right.
$$
Where $\alpha > 0$  and $\beta >0$
\subsection{Mean and Variance}
\subsubsection{Gamma}
$$\mu=\alpha \beta$$
$$\sigma^2 = \alpha \beta^2$$

\subsubsection{Exponential Distribution}
$$\mu = \beta $$
$$\sigma^2 = \beta^2$$
$$\sigma = \beta$$


\addtocounter{chapter}{1}
\chapter{Sampling Distribution}
\addtocounter{section}{2}
\section{Sampling Distribution}
The probability distribution of a statistic is called a sampling distribution. We are interested in the sampling distribution of the sample mean $\overline X$

The probability distribution of the sample means is called the sampling distribution of the sample mean.

\subsection{Properties of Sampling Distributions of Sample Means}
\begin{enumerate}
\item The mean of the sample means, $\mu_{\overline x}$ is equal to the population mean $\mu$.
\item The standard deviation of the sample means $\sigma_{\overline x}$
is equal to the population standard deviation  $\sigma$ divided by the square root of the sample size $n$.
$$\sigma_{\overline x} = \frac{\sigma}{\sqrt{n}}$$
This is called the standard error of the mean.
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\chapter{Homework}
\section{Homework Set 1}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
13 & 1.5, 1.6\\
17 & 1.11, 1.12\\
31 & 1.18, 1.19, 1.20, 1.29, 1.30\\
\end{tabular}

\section{Homework Set 2}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
42 & 2.3, 2.6, 2.10, 2.11, 2.14, 2.16, 2.18\\
51 & 2.24, 2.25, 2.28, 2.29, 2.30, 2.33, 2.34, 2.38, 2.39, 2.40, 2.44, 2.46, 2.47, 2.48\\
59 & 2.52, 2.53, 2.56, 2.57, 2.58, 2.62, 2.67, 2.70\\
\end{tabular}

\section{Homework Set 3}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
69 & 2.74, 2.75, 2.79, 2.82, 2.84, 2.87, 2.90, 2.92\\
\end{tabular}

\section{Homework Set 4}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
77 & 2.96, 2.97, 2.99, 2.100\\
91 & 3.1, 3.2, 3.5, 3.10, 3.11, 3.12, 3.25\\
\end{tabular}

\section{Homework Set 5}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
91 & 3.17, 3.27, 3.30, 3.31, 3.33, 3.35\\
104 & 3.37, 3.38 3.44, 3.45, 3.46, 3.52, 3.57\\
\end{tabular}

\section{Homework Set 6}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
106 & 3.53, 3.55, 3.58, 3.59, 3.60, 4.1, 4.2, 4.5, 4.7\\
117 & 4.10, 4.11, 4.13, 4.18, 4.19, 4.24, 4.28, 4.30, 4.34, 4.35, 4.49\\
127 & 4.39, 4.41, 4.44,4.45,4.46, 4.75, 4.77\\
\end{tabular}

\section{Homework Set 7}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
127 & 4.51\\
138 & 4.58, 4.60, 4.61, 4.62, 4.63, 4.70, 4.89\\
150 & 5.4, 5.6, 5.10, 5.17, 5.18, 5.19, 5.21,\\
157 & 5.32, 5.39, 5.43, 5.44\\
& 5.50, 5.52, 5.54, 5.57, 5.61, 5.66, 5.72\\
\end{tabular}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\nocite{textbook}
\printbibliography

\end{document}
