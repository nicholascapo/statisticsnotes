\documentclass{book}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                                                                                             %%
%%                                              Copyright 2012 Nicholas Capo                                             %%
%%                              This work is made available under the terms of the                             %%
%%                 Creative Commons Attribution-ShareAlike 3.0 Unported License.                   %%
%%                      http://creativecommons.org/licenses/by-sa/3.0/deed.en_US                       %%
%%                                                                                                                                              %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{setspace}
\usepackage{listings}
\usepackage{appendix}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[american]{babel}
\usepackage{pythontex}
\usepackage{graphicx}
\usepackage[hyperref=true, backref=true, backend=bibtex8]{biblatex}
\usepackage{csquotes}
\usepackage[pdftex, pdfusetitle, colorlinks, 
		urlcolor=blue, 
		filecolor=blue, 
		linkcolor=blue,
		citecolor=blue,]{hyperref}
\usepackage{datetime}
\settimeformat{ampmtime}

\bibliography{../textbook}

\title{\textsc{Class Notes\\ for \\ Statistics\\ Letu Math--3403}}
\author{Nicholas Capo\\ \href{mailto:nicholas.capo@gmail.com}{nicholas.capo@gmail.com}}

\date{\today\\ \currenttime}

\bibliography{textbook}

\newcommand{\note}[1]{\marginpar{\emph{Note: #1}}}

%%%%%%%%%%%%%%%
\begin{document}
\maketitle

\section*{}
\begin{tabular}{p{.8\textwidth}}
This document comprises classroom notes from {Statistics Class} at \href{letu.edu}{LeTourneau University}, in the Fall of 2012.\\

\vspace{5pt}

Although the author will attempt to be complete and correct in these notes, it is the reader's responsibility to learn and understand the material. The author assumes no responsibility for the completeness or accuracy of this document. \\

\vspace{20pt}

If you have any suggestions  or corrections feel free to email the author at \href{mailto:nicholas.capo@gmail.com}{nicholas.capo@gmail.com}\\

\vspace{5pt}

The latest version of this document is available at:\\ \url{https://bitbucket.org/nicholascapo/statisticsnotes/src/tip/StatisticsNotes.pdf}\\

\vspace{5pt}

You may also view the \LaTeX\ source code at\\
\url{https://bitbucket.org/nicholascapo/statisticsnotes}\\
\end{tabular}

\vfill

\begin{tabular}{c}
\href{http://creativecommons.org/licenses/by-sa/3.0}{\includegraphics[width=.25\textwidth]{by-sa.png}}\\
Copyright \copyright\ 2012 Nicholas Capo\\
This work is made available under the terms of the\\
Creative Commons Attribution-ShareAlike 3.0 Unported License.\\
\url{http://creativecommons.org/licenses/by-sa/3.0/deed.en_US}\\
\end{tabular}

%%%%%%%%%%%
\tableofcontents
%%%%%%%%%%%

\chapter{Introduction}
\begin{center}
\textbf{Definition of Statistics}\\
\enquote{Statistics is the science of collecting, organizing, analyzing, and interpreting data in order to make decisions.}
\end{center}

\section{Data}

\subsection{Data Sets}
\begin{description}
\item[Population] The collection of all outcomes, responses, measurements, or counts, that are of interest.

\item[Sample] A subset of the population.

\item[Parameter] A number that describes a population characteristic.

\item[Statistic] A number that describes a sample characteristic.
\end{description}

\subsection{Types of Data}

\begin{description}
\item[Qualitative Data] Attributes, labels, or non-numerical entries.

\item[Quantitative Data] Numerical measurements or counts.
\end{description}

\section{Sample Mean and Median}

\subsection{Definition}
\begin{description}
\item[Sample Mean] The average of the sample data points, however it may not be a data point.
$$\overline{x} = \sum_{i=1}^n\frac{x_i}{n} = \frac{x_1+x_2+x_3\cdots x_n}{n}$$
\item[Sample Median] The middle value of the data.

$$\tilde{x}=\left\{
\begin{matrix}
x_{(\frac{n+1}{2})} & \text{if $n$ is odd}\\
\frac{1}{2}(x_{\frac{n}{2}}+x_{\frac{n}{2}+1}) & \text{if $n$ is even}
\end{matrix}
\right.$$

\item[Trimmed Mean] A trimmed mean is computed by trimming off the largest and smallest set of values. For example a 10\% trimmed mean is found by eliminating the largest 10\% and smallest 10\% and computing the mean of the remaining values. This may be useful for data that contains possible outliers. Denoted by $x_{tr(\text{percent})}$
\end{description}

\section{Measures of Variability}

\subsection{Standard Deviation}

\subsubsection{Sample Variance}

$$s^2 = \sum_{i=1}^n \frac{(x_i - \overline{x})^2}{n-1}$$

\subsubsection{Sample Standard Deviation}

$$s=+\sqrt{s^2}$$

The standard deviation is $0$ when all the data points are the same.

\section{Descriptive Statistics}

\subsection{Quartiles}

Quartiles approximately divide an ordered data set into four equal parts.

\begin{description}
\item[First Quartile, $Q_1$]
About $25\%$ of the data fall on or below $Q_1$
\item[Second Quartile, $Q_2$]
About $50\%$ of the data fall on or below $Q_2$
\item[Third Quartile, $Q_3$]
About $75\%$ of the data fall on or below $Q_3$
\end{description}

\subsection{Range and Interquartile Range}

\subsubsection{Range}

$$\text{range} = \text{max value} - \text{min value}$$

\subsubsection{Interquartile Range}

$$IQR=Q_3 - Q_1$$

To help find outliers, compute $1.5 \times IQR$, and any values that lie outside the interval $[Q_1-1.5 \times IQR, Q_3+1.5 \times IQR]$ is a possible (and probable) outlier.

\subsection{Box and Whisker Plot}

Exploratory Data Analysis Tool

\begin{itemize}
\item Requires
	\begin{itemize}
	\item Min
	\item $Q_1$
	\item Median
	\item $Q_3$
	\item Max
	\end{itemize}
\end{itemize}

\begin{pycode}
import pylab
data = [1, 2, 3, 4, 5, 6, 11]
pylab.figure(figsize=(5,2))
pylab.boxplot(data, vert=0, sym='bx')
pylab.savefig('whiskerplot.pdf', bbox_inches='tight', orientation='landscape')
sdata = sorted(data)
min = sdata[0]
outlier = sdata[-1]
max = sdata[-2]
median = pylab.median(sdata)

\end{pycode}

\subsubsection{Example}

\begin{tabular}{ll}
Example Data &\py{data}\\
Min &\py{min}\\
Median & \py{median}\\
Max & \py{max}\\
Outlier & \py{outlier}\\
\end{tabular}


\begin{figure}[H]
\begin{center}
\includegraphics[width=.75\textwidth]{whiskerplot}
\end{center}
\caption{Example Box And Whisker Plot}
\end{figure}

\section{Stem and Leaf Plots}

These look like a sideways histogram

Data: \py{[31, 21, 32, 33, 41, 42, 58, 25, 21]}\\

\begin{tabular}{r|ll}
Stem & Leaf & Key: $a|b=ab$\\
\hline
2&1,1,5\\
3&1,2,3\\
4&1,2\\
5&8\\
\end{tabular}

\subsection{Key Notation}

Key: 4|5 = 45
Key: 4|5 = 4.5

\subsection{Double Stem and Leaf}

Separate the leaves into two groups, (0-4, and 5-9)

Data: \py{[31, 21, 32, 33, 41, 42, 58, 25, 21]}\\

\begin{tabular}{r|ll}
Stem & Leaf & Key: $a|b=ab$\\
\hline
2&1,1\\
2&5\\
3&1,2,3\\
4&1.2\\
4&\\
5&\\
5&8\\
\end{tabular}

\section{Frequency Distribution}
A table that shows classes or intervals of data with a count of the number of entries in each class.

\subsection{Midpoint of a Class}
Average of the class limits.
$$\frac{(\text{lower class limit})+(\text{upper class limit})}{2}$$

\subsection{Relative Frequency}
$$\frac{\text{class frequency}}{\text{sample size}}=\frac{f}{n}$$

\section{Scatter Plots}
Each entry in one data set corresponds to one entry in a second set, one-to-one mapping.

\subsection{Example Scatter Plot}

\begin{pycode}
import pylab
import random
count = 12
data = []
for i in range(count): data.append(random.randint(1, count))
x = range(1, count+1)

pylab.figure(figsize=(5, 5))

pylab.scatter(x, data, label='$(x, y)$')

pylab.legend()
pylab.savefig('scatter.pdf', orientation='landscape')
\end{pycode}

\begin{tabular}{ll}
Data:&\\
X: & \texttt{\py{x}}\\
Y: & \texttt{\py{data}}\\
\end{tabular}

\begin{figure}[H]
\begin{center}
\includegraphics[width=.75\textwidth]{scatter}
\end{center}
\caption{Example Scatter Plot}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Probability}

\section{Experiments}
Any process that generates a set of data.

\section{Sample Space}
The set of all possible outcomes of a statistical experiment, denoted $S$. The sample space with no elements is the empty set or null set, denoted $\emptyset$

\subsection{Example}

$$S = \{ 3, 2, 1, 0\}$$
$$S = \{ x | 0 < x < 25 \}$$
$$S= \{ x^2 | x \in \mathbb{R}\}$$

\subsection{Tree Diagrams}
A Tree Diagram can be used to list all possible outcomes

\subsection{Events}
An event is a subset of a sample space. The null set ($\emptyset$) and the sample space ($S$) are both subsets of the sample space $S$.

\subsubsection{Intersection}
The intersection of two events $A$ and $B$, denoted $A \cap B$, is the event containing all elements that are common to $A$ and $B$. If $A \cap B = \emptyset$ than $A$ and $B$ are called mutually exclusive or disjoint.

\subsubsection{Union}
The union of two events $A$ and $B$, denoted $A \cup B$, is the event containing all elements that belong to $A$ or $B$ or both.

\subsubsection{Compliment}
The compliment of an event $A$ with respect to $S$ is a subset of all elements of $S$ not in $A$, denoted $A'$

\section{Counting Sample Points}

\subsection{Multiplication Rule}
If an operation can be preformed in $n_1$ ways and if for each of the ways a second operation can be preformed in $n_2$ ways, then the two operations can be preformed together in $n_1n_2$ ways. This principle can be extended to more than two operations. See Example 2.14 in \textcite[45]{textbook}

\subsection{Factorial}
For any non-negative integer $n$, $n!$ called \enquote{n factorial}, is defined as $$n!=n(n-1)\cdots(2)(1)$$
with the special case $0!=1$.

\subsection{Permutation}
A permutation is an arrangement of all or a part of a set objects. For permutations the order of objects matters. The number of permutations of $n$ distinct objects is $n!$.

\subsection{Permutations at a Time}
The number of permutations of $n$ distinct objects taken $r$ at a time is
$${}_nP_r = \frac{n!}{(n-r)!}$$
\note{This is called \enquote{$n$ Permute $r$}}

\subsection{Permutations in a Circle}
The number of permutations of $n$ objects arranged in a circle is $(n-1)!$.

\subsection{Permutations of a Kind}
The number of distinct permutations of $n$ objects of which $n_1$ are of one kind, $n_2$ of a second kind, \ldots , $n_k$ of a $k$th kind is

$$\frac{n!}{n_1!n_2! \cdots n_k!}$$

\subsection{Partitioning}
The number of way of partitioning a set of $n$ objects into $k$ cells with $n_1$ elements in the first cell, $n_2$ elements in the second cell, and so forth, is
$$ {n \choose n_1, n_2, \ldots , n_k }= \frac{n!}{n_1!n_2! \cdots n_k!}$$
Where $n_1+n_2+\cdots+n_k = n$

\note{This is the same as the last example}

\subsection{Combinations}

The number of combinations of $n$ distinct object taken $r$ at a time, is:
$${}_nC_r={n \choose r} = \frac{n!}{r!(n-r)!}$$
\note{This is partitioning with only two cells}

\subsection{Partitioning and Combinations}

Note that:

$$ {10 \choose 5, 4, 1} = {10 \choose 5} {5 \choose 4} {1 \choose 1}$$


%%%%%%%%%
\section{Probability of an Event}

The probability of an event $A$ is the sum of the weifhts of all sample points on $A$. Therefore $$0 \le P(A) \le 1$$ $$P(\emptyset)=0$$ $$P(S)=1$$

Furthermore, if $A_1, A_2, \cdots$ is a set of mutually exclusive events, then $$P(A_1 \cup A_2 \cup \cdots)=P(A_1)+P(A_2)+\cdots$$

\subsection{Complimentary Probability}
If $A$ and $A'$ are complimentary events, then $$P(A)+P(A')=1$$

\subsection{Different Outcomes}

If an experiment can result in any one of $N$ different equally likely outcomes, and of exactly $n$ of those outcomes correspond to event $A$, the probability of event $A$ is $$P(A) = \frac{n}{N}$$

\subsection{Additive Rule of Probability}

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

\section{Conditional Probability}

The conditional probability of $B$ given $A$, denoted $P(B|A)$ is defined as $$P(B|A) = \frac{P(A \cap B)}{P(A)} \text{, where } P(A) \ne 0 $$
\note{This is a filter}

\subsubsection{Alternate Notation}
$$P(A|B) =\frac{n(A \cap B)}{n(B)}$$
Where $n(x)$ is defined as \enquote{the number of elements in $x$} 

\subsection{Multiplication Rule}
If in an experiment the events $A$ and $B$ can both occur, then $$P(A\cap B) = P(A)P(B|A)\text{, provided } P(A) > 0$$

\subsection{Independent Events}
Two events $A$ and $B$ are \emph{independent} if and only if 
$$P(B|A) = P(B) \text{ or } P(A|B)=P(A)$$
Assuming the existence of the conditional probabilities. Otherwise, $A$ and $B$ are dependent.

\note{Independence does not imply Mutual Exclusivity!}

\subsection{Corollary}
Two events $A$ and $B$ are \emph{independent} if and only if 
$$P(A \cap B) = P(A)P(B)$$

Therefore, to obtain the probability that two independent events will occur, we simply find the product of their individual probabilities.

\subsection{Compliments of Independent Events}
If $A$ and $B$ are independent events so are the complements of these events, this means that: 
\begin{itemize}
\item $A'$ and $B'$ are independent
\item $A'$ and $B$  are independent
\item $A$ and $B'$ are independent
\end{itemize}
\note{Also $P(A'|B) = 1 - P(A|B)$ (no need for independence)}

\subsection{Extended Multiplication for Conditional Probability}
If, in an experiment, the events $A_1, A_2 \cdots A_k$ can occur, then
$$P(A_1 \cap A_2 \cap \cdots \cap A_k) = P(A_1) \times P(A_2|A_1) \times P(A_3|A_1 \cap A_2) \times \cdots \times P(A_k |  A_1 \cap A_2 \cap \cdots \cap A_{k-1})$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayes' Rule}

\subsection{Partitioning the Sample Space}
A sample space $A$ can be partitioned into an arbitrary number of subsets. For partitions $B_1, B_2, B_3$ then $$A = (B_1 \cap A) \cup (B_2 \cap A) \cup \cdots \cup (B_3 \cap A)$$

\subsection{Theorem of Total Probability}
$$P(A) = \sum_{i=1}^k P(B_i \cap A) = \sum_{i=1}^k P(B_i)P(A|B_i)$$

\subsection{Bayes Rule}
$$P(B_i|A) 
= \frac{P(B_i \cap A)}{\sum_{i=0}^k P(B_i \cap A)} 
= \frac{P(B_i \cap A)}{\sum_{i=0}^k P(B_i)P(A|B)}$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Random Variables}

\section{Concept of a Random Variable}

\subsection{Definition}
A random variable, denoted $X$, is a function that associates a real number with each element in the sample space.

\subsection{Discrete Sample Space}
If a sample space contains a finite number of possibilities or an unending sequence with as many elements as there are whole numbers, it is called a \emph{discrete sample space}.

A random variable corresponding with a discrete sample space is called a \emph{discrete random variable}.

\subsection{Continuous Sample Space}
If a sample space contains an infinite number of possibilities equal to the number of points on a line segment, it is called a \emph{continuous sample space}.

A random variable corresponding with a continuous sample space is called a \emph{continuous random variable}.

\section{Discrete Probability Distribution}
The set of ordered pairs $(x, f(x))$ is a \emph{probability function}, \emph{probability mass function}, or \emph{probability distribution} of the discrete random variable $X$ if, for each possible outcome $x$, 
\begin{enumerate}
\item $f(x) \ge 0$
\item $\sum_x f(x) = 1$
\item $P(X = x) = f(x)$
\end{enumerate}
\note{Probabilities are Functions!} 

\subsection{Cumulative Distribution Function}

The Cumulative Distribution Function $F(x)$ of a discrete random variable$X$ with probability distribution $f(x)$ is
$$F(x)=P(X \le x) = \sum_{t\le x}F(t)\text{, for } -\infty < x < \infty$$

\note{This keeps a running total of the cumulative probabilities up to a value}

\section{Continuous Random Variables}

\subsection{Probability Density Function}
The function $f(x)$ is called a probability density function or pdf for the continuous random variable $X$, defined over the set of real numbers, if
\begin{enumerate}
\item $$F(x) \ge 0 \text{, for all } x \in R$$
\item $$\int_{-\infty}^{\infty}f(x) \text{dx} = 1$$
\item $$P(a < X < b) = \int_a^b f(x) \text{dx}$$
\end{enumerate}
\note{$P(X=c)=0$, where $a \le c \le b$}

\subsection{Cumulative Distribution Function}
The Cumulative Distribution Function $F(x)$ of a continuous random variable $X$ with density function $f(x)$ is
$$F(x) = P(X \le x) = \int_{-\infty}^x f(t) \text{ dt, for } -\infty < x < \infty$$

\note{$P(a<X<b)=F(a)-F(b)$}
\note{$f(x)=F'(x)$}

\subsection{Joint Probability Mass Functions}
The function $f(x,y)$ is a Joint Probability Function or Probability Mass Function of the discrete random variables $X$ and $Y$ if
\begin{enumerate}
\item $$f(x,y) \ge 0 \text{ for all } (x,y)$$
\item $$\sum_x\sum_y f(x,y) =1 $$
\item $$P(X=x, Y=y)=f(x,y)$$
\item For any region $A$ in the $xy$ plane, $$P((X,Y) \in A) = \sum_A\sum f(x,y)$$

\end{enumerate}

\section{Joint Probability Distributions}
he function $f(x,y)$ is a joint density function of the continuous random variables $X$ and $Y$ if
\begin{enumerate}
\item $$f(x,y) \ge 0 \text{, for all} (x,y)$$
\item $$\int_{-\infty}^\infty \int_{-\infty}^\infty f(x,y) \text{dx dy} = 1$$
\item $$P((X,Y) \in A) = \int_A \int f(x,y) \text{dx dy}$$
\end{enumerate}

\subsection{Marginal Distribution}
The marginal distribution of $X$ alone $Y$ alone are 

(for the discrete case)
$$g(x) = \sum_y f(x,y) \text{  and  } h(y) = \sum_x f(x,y)$$

(for the continuous case)
$$g(x) = \int_{-\infty}^\infty f(x,y) \text{dy} \text{  and  } h(y) = \int_{-\infty}^\infty f(x,y) \text{dx}$$

\note{Marginal distributions are probability distributions}

\section{Joint PDFs}
Let $X$ and $Y$ be two random variables, discrete or continuous. The conditional distribution of the random variable $Y$ given that $X=x$ is
$$f(y|x) = \frac{f(x,y)}{g(x)} \text{, provided } g(x) > 0$$

Similarly, the conditional distribution of the random variable $X$ given that $Y=y$ is
$$f(y|x) = \frac{f(x,y)}{h(y)} \text{, provided } h(y) > 0$$

Where $g(x)$ and $h(y)$ are the respective marginal distributions.

\subsection{Statistical Independence}
Let $X$ and $Y$ be two random variables, discrete or continuous, with joint probability distribution $f(x,y)$ and marginal distributions $g(x)$ and $h(y)$ respectively. The random variables $X$ and $Y$ are said to be statistically independent if and only if
$$f(x,y)=g(x)h(y)$$
for all $(x,y)$ withing their range.

\subsubsection{Statistical Independence (Extended Case)}
Let $X_1, X_2, \cdots , X_n$ be n random variables, discrete or continuous, with joint probability distribution $f(x_1, x_2, \cdots , x_n)$ and marginal distributions $f_1(x_1), f_2(x_2), \cdots , f_n(x_n)$. The random variables $X_1, X_2, \cdots , X_n$ are said to be statistically independent if and only if
$$f(x_1, x_2, \cdots, x_n) = f_1(x_1), f_2(x_2), \cdots , f_n(x_n)$$
for all $(x,y)$ withing their range.

%%%%%%%%%%%%%%%%%%
\chapter{Mean of a Random Variable}

\section{Mean of a Random Variable}
Let $X$ be a random variable with probability distribution $f(x)$. The mean, or expected value, of $X$ is:

Discrete:
	$$\mu = E(X) = \sum_x x f(x)$$ 

Continuous:
	$$\mu = E(X) = \int_{=\infty}^\infty x f(x) \text{dx}$$


\subsection{Expected Values of Functions of Random Variables}

Let $X$ be a random variable with pdf $f(x)$. The expected value of the random variable $g(X)$ is

Discrete:
$$E(g(X))=\sum_x g(x)f(x)$$

Continuous:
$$E(g(X))=\int_{-\infty}^\infty g(x)f(x)$$

\subsection{Expected Values of Joint distributions}
Let $X$ and $Y$ be random variables with joint probability distribution $f(x,y)$. The mean, or expected value, of a random random variable $g(x,y)$ id

Discrete:
$$\mu_{g(x,y)}=E[g(x,y)] = \sum_X \sum_Y g(x,y)f(x,y)$$

Discrete:
$$\mu_{g(x,y)}=E[g(x,y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y)f(x,y)$$

\section{Variance and Covariance of Random Variables}

\subsection{Variance/Standard Deviation}

Variance/Standard Deviation is a measure of the typical (average) amount the value deviates from the mean.

The further the values are from the mean, the grater the variance/standard variation. 

\subsection{Variance of a Random Variable}
Let $X$ be a random variable with probability distribution $f(x)$ and mean $\mu$. The variance of $X$ is 

Discrete:
$$\sigma^2=E[(X-\mu)^2]=\sum_x(x-\mu)^2 f(x)$$

$$\sigma^2=E[(X-\mu)^2]=\int_{-\infty}^\infty (x-\mu)^2 f(x)$$

\subsubsection{Standard Variation}

$$\sigma = \sqrt{\sigma^2} = \sqrt{E((X-\mu)^2)}$$

\begin{itemize}
\item Variance --- In terms of the mean's unit squared
\item Standard Deviation --- Terms of the mean's unit
\item It does not makes sense to add or subtract the variance and the mean
\item It does not make sense to add or subtract the standard deviation and the mean
\end{itemize}

\subsection{Variance of a Random Variable}
The variance of a Random Variable $X$ is
$$\sigma^2 = E(X^2)-\mu^2$$

\subsection{The Epirical Rule}
for data with a (symmetric) bell-shaped distribution,
\begin{itemize}
\item $P[(\mu -\sigma) < X < (\mu + \sigma)] \approx 68\%$
\item $P[(\mu -2\sigma) < X < (\mu + 2\sigma)] \approx 95\%$
\item $P[(\mu -3\sigma) < X < (\mu + 3\sigma)] \approx 99.7\%$
\end{itemize}

\subsection{Chebyshev's Theorem}

The probability that any random variable $X$ will assume a value within $k$ standard deviations of the mean is at least $1-\frac{1}{k^2}$. That is,
$$P[(\mu - k\sigma) < X < (\mu + k\sigma)] \ge 1 - \frac{1}{k^2}$$

\subsection{Variance of a Function of a Random Variable}
Let $X$ be a random variable with probability distribution $f(x)$. The variance of the random variable $g(X)$ is

Discrete:
$$\sigma^2_{g(X)} = E\{[g(X) - \mu_{g(X)}]^2\} = \sum_x [g(x) - \mu_{g(X)}]^2 f(x)$$

Continuous:
$$\sigma^2_{g(X)} = E\{[g(X) - \mu_{g(X)}]^2\} = \int_{-\infty}^\infty [g(x) - \mu_{g(X)}]^2 f(x) \text{dx}$$

\subsection{Alternative Form of Variance}
$$\sigma^2 = E[[g(x)]^2]-\mu^2_{g(x)}$$

\subsection{Covariance of $X$ and $Y$}
Let $X$ and $Y$ be random variable with joint pdf  $f(x,y)$. The covariance of $X$ and $Y$ is

Discrete:
$$\sigma_{XY} = E[(X-\mu x)(Y-\mu y)]=\sum_X \sum_Y (x-\mu x)(y-\mu y) f(x,y)$$

Continuous:
$$\sigma_{XY} = E[(X-\mu x)(Y-\mu y)]=\int_{-\infty}^\infty \int_{-\infty}^\infty (x-\mu x)(y-\mu y) f(x,y) \text{dx dy}$$

\subsection{Alternate Form}
$$\sigma_{XY}=E(XY) - \mu x \mu y$$

\subsection{Description of Covariance}
\textbf{[See Slide 4 -- 26]}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\chapter{Homework}
\section{Homework Set 1}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
13 & 1.5, 1.6\\
17 & 1.11, 1.12\\
31 & 1.18, 1.19, 1.20, 1.29, 1.30\\
\end{tabular}

\section{Homework Set 2}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
42 & 2.3, 2.6, 2.10, 2.11, 2.14, 2.16, 2.18\\
51 & 2.24, 2.25, 2.28, 2.29, 2.30, 2.33, 2.34, 2.38, 2.39, 2.40, 2.44, 2.46, 2.47, 2.48\\
59 & 2.52, 2.53, 2.56, 2.57, 2.58, 2.62, 2.67, 2.70\\
\end{tabular}

\section{Homework Set 3}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
69 & 2.74, 2.75, 2.79, 2.82, 2.84, 2.87, 2.90, 2.92\\
\end{tabular}

\section{Homework Set 4}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
77 & 2.96, 2.97, 2.99, 2.100\\
91 & 3.1, 3.2, 3.5, 3.10, 3.11, 3.12, 3.25\\
\end{tabular}

\section{Homework Set 5}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
91 & 3.17, 3.27, 3.30, 3.31, 3.33, 3.35\\
104 & 3.37, 3.38 3.44, 3.45, 3.46, 3.52, 3.57\\
\end{tabular}

\section{Homework Set 6}
\begin{tabular}{r|l}
Page & Problem Numbers\\
\hline
?? & 3.53, 3.55, 3.58, 3.59, 3.60, 4.1, 4.2, 4.5, 4.7\\
?? & 4.10, 4.11, 4.13, 4.18, 4.19, 4.24, 4.28, 4.30, 4.34, 4.35, 4.49\\
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\nocite{textbook}
\printbibliography

\end{document}
